{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.transformers import NeuralRegressionTransformer\n",
    "from proglearn.voters import NeuralRegressionVoter\n",
    "from proglearn.deciders import KNNRegressionDecider, NeuralRegressionDecider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook test the progressive learning regression methodology on a few toy multitasks with known outcomes, to see if it can successfully learn and share representations. In all cases, three tasks are presented. Each are regression of a single real-valued response on a real-valued, $d$ dimensional vector. In all cases, the vector will be scaled, have some known transformation applied to it (common for all tasks), and then another vote-decision function that maps it to the output (which differs for each task). The following three multitask then differ on the following five parameters: how the input is sampled, which common transformer, and which vote-decision to use for each of the three subtasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n is the per-task sample size.\n",
    "def generate_data(n, d, transform, vote, sigma = 1):\n",
    "    X = np.random.randn(n, d)\n",
    "    y = vote(transform(X)) + np.random.normal(loc=0.0, scale=sigma, size=n)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def plot_data(data):\n",
    "    sns.set(font_scale = 1.5)\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (5,4))\n",
    "    \n",
    "    colors = [\"#c51b7d\", \"#2166ac\", \"#d95f02\"]\n",
    "    \n",
    "    for (X, y), color in zip(data, colors):\n",
    "        if X.shape[1] != 1:\n",
    "            raise ValueError(\"X must be one-dimensional to plot.\")\n",
    "        \n",
    "        ax.scatter(X, y, color = color, marker = \".\")    \n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        \n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = check_array(y_true, ensure_2d=False), check_array(y_pred, ensure_2d=False)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 100\n",
    "d = 1\n",
    "\n",
    "rep_dim = 29\n",
    "W = np.random.randn(rep_dim, d)\n",
    "b = np.random.randn(rep_dim)\n",
    "v1 = np.random.randn(rep_dim)\n",
    "v2 = np.random.randn(rep_dim)\n",
    "v3 = np.random.randn(rep_dim)\n",
    "vs = [v1, v2, v3]\n",
    "\n",
    "data = []\n",
    "for v in vs:\n",
    "    data.append(generate_data(n, d, lambda X: np.square(np.dot(X, W.T) + b), lambda X: np.dot(X, v), sigma = 5))\n",
    "\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a PL that has a neural transformer, a neural voter (with one output unit), and a KNN decider.\n",
    "def build_predictive_ensembler(d, verbose_transform = False, verbose_vote = False):    \n",
    "    \n",
    "    default_transformer_class = NeuralRegressionTransformer\n",
    "        \n",
    "    # Representation\n",
    "    network = keras.Sequential()\n",
    "    network.add(Input(shape=(d,), name = 'input'))\n",
    "    network.add(Dense(16, activation='relu', name = 'fc0'))\n",
    "    network.add(Dense(4, activation='relu', name = 'fc1')) \n",
    "    \n",
    "    # Output layer.\n",
    "    network.add(Dense(1, activation = 'linear', name = 'output'))\n",
    "\n",
    "    default_transformer_kwargs = {\"network\" : network, \n",
    "                                  \"euclidean_layer_idx\" : -2,\n",
    "                                  \"loss\" : \"mae\",\n",
    "                                  \"optimizer\" : keras.optimizers.Adam(3e-2),\n",
    "                                  \"fit_kwargs\" : {\"epochs\" : 100, \"verbose\" : verbose_transform}\n",
    "                                 }\n",
    "\n",
    "    default_voter_class = NeuralRegressionVoter\n",
    "    default_voter_kwargs = {\"loss\" : \"mae\",\n",
    "                            \"verbose\" : verbose_vote,\n",
    "                            \"epochs\" : 100,\n",
    "                            \"lr\" : 3e-2\n",
    "                           }\n",
    "    default_decider_class = KNNRegressionDecider\n",
    "    \n",
    "    return ProgressiveLearner(default_transformer_class = default_transformer_class, \n",
    "                              default_transformer_kwargs = default_transformer_kwargs,\n",
    "                              default_voter_class = default_voter_class,\n",
    "                              default_voter_kwargs = default_voter_kwargs, \n",
    "                              default_decider_class = default_decider_class,\n",
    "                              default_decider_kwargs = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a PL that has a neural transformer, a neural voter (with one output unit), and a KNN decider.\n",
    "def build_representation_ensembler(d, num_tasks = 3, verbose_transform = False, verbose_decide = False):    \n",
    "    \n",
    "    default_transformer_class = NeuralRegressionTransformer\n",
    "    \n",
    "    backbone_layer_size = 4 # second to last layer in each transformer and the full decider.\n",
    "        \n",
    "    # Representation\n",
    "    transform_network = keras.Sequential()\n",
    "    transform_network.add(Input(shape=(d,), name = 'input'))\n",
    "    transform_network.add(Dense(16, activation='relu', name = 'fc0'))\n",
    "    transform_network.add(Dense(backbone_layer_size, activation='relu', name = 'fc1')) \n",
    "    \n",
    "    # Output layer.\n",
    "    transform_network.add(Dense(1, activation = 'linear', name = 'output2'))\n",
    "\n",
    "    default_transformer_kwargs = {\"network\" : transform_network, \n",
    "                                  \"euclidean_layer_idx\" : -2,\n",
    "                                  \"loss\" : \"mae\",\n",
    "                                  \"optimizer\" : keras.optimizers.Adam(3e-2),\n",
    "                                  \"fit_kwargs\" : {\"epochs\" : 100, \"verbose\" : verbose_transform}\n",
    "                                 }\n",
    "\n",
    "    # Unused in this model.\n",
    "    default_voter_class = NeuralRegressionVoter\n",
    "    default_voter_kwargs = {}\n",
    "    \n",
    "    default_decider_class = NeuralRegressionDecider\n",
    "    \n",
    "    decide_network = keras.Sequential()\n",
    "    decide_network.add(Input(shape=(backbone_layer_size * num_tasks), name = 'concatenated_input3'))\n",
    "    decide_network.add(Dropout(0.3, name = 'd4'))\n",
    "    decide_network.add(Dense(backbone_layer_size, activation='relu', name = 'fc5'))\n",
    "    \n",
    "    # Output layer.\n",
    "    decide_network.add(Dense(1, activation = 'linear', name = 'output6'))\n",
    "    \n",
    "    default_decider_kwargs = {\"network\" : decide_network,\n",
    "                              \"loss\" : \"mae\",\n",
    "                              \"optimizer\" : keras.optimizers.Adam(3e-2),\n",
    "                              \"fit_kwargs\" : {\"epochs\" : 100, \"verbose\" : verbose_decide}\n",
    "                             }\n",
    "    \n",
    "    return ProgressiveLearner(default_transformer_class = default_transformer_class, \n",
    "                              default_transformer_kwargs = default_transformer_kwargs,\n",
    "                              default_voter_class = default_voter_class,\n",
    "                              default_voter_kwargs = default_voter_kwargs, \n",
    "                              default_decider_class = default_decider_class,\n",
    "                              default_decider_kwargs = default_decider_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check 1\n",
    "In this task, we transform by applying a random linear transform followed by an element-wise square. We vote by a random linear combination and add noise. Here, we just wish to show that the Progressive Learner is a single-task learner, in that it can learn one task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 10000\n",
    "n_test = 1000\n",
    "d = 10\n",
    "\n",
    "np.random.seed(1)\n",
    "rep_dim = 4 # The matrix will be invertible with high probabability.\n",
    "W = (1 / d) * np.random.randn(rep_dim, d)\n",
    "b = np.random.randn(rep_dim)\n",
    "v = np.random.randn(rep_dim)\n",
    "\n",
    "# We will do no noise to see that it can fit a deterministic function.\n",
    "X, y = generate_data(n_train + n_test, d, lambda X: np.square(np.dot(X, W.T) + b), lambda X: np.dot(X, v), sigma = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=n_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7193 - MAPE: 46.8426 - MAE: 2.7193\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3127 - MAPE: 23.3172 - MAE: 1.3127\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1423 - MAPE: 21.9733 - MAE: 1.1423\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1048 - MAPE: 21.8821 - MAE: 1.1048\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1116 - MAPE: 23.0179 - MAE: 1.1116\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.1095 - MAPE: 23.6402 - MAE: 1.1095\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1063 - MAPE: 24.1249 - MAE: 1.1063\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0645 - MAPE: 23.5095 - MAE: 1.0645\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0431 - MAPE: 22.6446 - MAE: 1.0431\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0986 - MAPE: 23.6874 - MAE: 1.0986\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1072 - MAPE: 24.5801 - MAE: 1.1072\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0891 - MAPE: 24.4189 - MAE: 1.0891\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1174 - MAPE: 25.0345 - MAE: 1.1174\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0902 - MAPE: 24.4678 - MAE: 1.0902\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0824 - MAPE: 24.3656 - MAE: 1.0824\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1358 - MAPE: 24.8278 - MAE: 1.1358\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1078 - MAPE: 24.4806 - MAE: 1.1078\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1352 - MAPE: 25.2646 - MAE: 1.1352\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0908 - MAPE: 24.5563 - MAE: 1.0908\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1141 - MAPE: 24.5506 - MAE: 1.1141\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1204 - MAPE: 25.1467 - MAE: 1.1204\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0967 - MAPE: 24.5847 - MAE: 1.0967\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1059 - MAPE: 24.5114 - MAE: 1.1059\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0851 - MAPE: 24.2155 - MAE: 1.0851\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0900 - MAPE: 24.1984 - MAE: 1.0900\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0807 - MAPE: 24.2092 - MAE: 1.0807\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1114 - MAPE: 24.6582 - MAE: 1.1114\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0889 - MAPE: 24.6671 - MAE: 1.0889\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1260 - MAPE: 24.8198 - MAE: 1.1260\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1302 - MAPE: 25.3451 - MAE: 1.1302\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1510 - MAPE: 25.3130 - MAE: 1.1510\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1066 - MAPE: 25.2065 - MAE: 1.1066\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0914 - MAPE: 24.6843 - MAE: 1.0914\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0867 - MAPE: 24.2944 - MAE: 1.0867\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0768 - MAPE: 24.0559 - MAE: 1.0768\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1023 - MAPE: 24.3197 - MAE: 1.1023\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0727 - MAPE: 24.1754 - MAE: 1.0727\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0929 - MAPE: 24.4249 - MAE: 1.0929\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0864 - MAPE: 24.4120 - MAE: 1.0864\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0893 - MAPE: 24.4100 - MAE: 1.0893\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1024 - MAPE: 24.1753 - MAE: 1.1024\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0927 - MAPE: 24.3730 - MAE: 1.0927\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1192 - MAPE: 24.8056 - MAE: 1.1192\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0974 - MAPE: 24.3141 - MAE: 1.0974\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0858 - MAPE: 24.4101 - MAE: 1.0858\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0879 - MAPE: 24.4877 - MAE: 1.0879\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1010 - MAPE: 24.6063 - MAE: 1.1010\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1358 - MAPE: 25.1557 - MAE: 1.1358\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.1075 - MAPE: 24.9433 - MAE: 1.1075\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0842 - MAPE: 24.5735 - MAE: 1.0842\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0682 - MAPE: 23.8450 - MAE: 1.0682\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0950 - MAPE: 24.6439 - MAE: 1.0950\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0834 - MAPE: 24.3722 - MAE: 1.0834\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 819us/step - loss: 1.0788 - MAPE: 24.1044 - MAE: 1.0788\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0883 - MAPE: 24.1512 - MAE: 1.0883\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0824 - MAPE: 24.1081 - MAE: 1.0824\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0472 - MAPE: 23.6498 - MAE: 1.0472\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0520 - MAPE: 23.6549 - MAE: 1.0520\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0283 - MAPE: 23.1481 - MAE: 1.0283\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0377 - MAPE: 22.9406 - MAE: 1.0377\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0322 - MAPE: 22.9806 - MAE: 1.0322\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0656 - MAPE: 23.2126 - MAE: 1.0656\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0129 - MAPE: 22.3394 - MAE: 1.0129\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0412 - MAPE: 22.3152 - MAE: 1.0412\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.9350 - MAPE: 20.6729 - MAE: 0.9350\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.8417 - MAPE: 18.8522 - MAE: 0.8417\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7537 - MAPE: 17.5782 - MAE: 0.7537\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7344 - MAPE: 17.1244 - MAE: 0.7344\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7388 - MAPE: 16.9205 - MAE: 0.7388\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7048 - MAPE: 16.6614 - MAE: 0.7048\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7608 - MAPE: 17.5513 - MAE: 0.7608\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7594 - MAPE: 17.4526 - MAE: 0.7594\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7899 - MAPE: 17.9368 - MAE: 0.7899\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7169 - MAPE: 17.0927 - MAE: 0.7169\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7673 - MAPE: 17.5259 - MAE: 0.7673\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7186 - MAPE: 17.0537 - MAE: 0.7186\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7400 - MAPE: 17.2506 - MAE: 0.7400\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7334 - MAPE: 16.8670 - MAE: 0.7334\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7200 - MAPE: 16.8911 - MAE: 0.7200\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7589 - MAPE: 17.1478 - MAE: 0.7589\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7237 - MAPE: 16.8553 - MAE: 0.7237\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7197 - MAPE: 16.7756 - MAE: 0.7197\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7571 - MAPE: 17.4472 - MAE: 0.7571\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7222 - MAPE: 16.8879 - MAE: 0.7222\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7211 - MAPE: 16.9187 - MAE: 0.7211\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7094 - MAPE: 16.5054 - MAE: 0.7094\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7486 - MAPE: 17.3864 - MAE: 0.7486\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7455 - MAPE: 17.3001 - MAE: 0.7455\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7498 - MAPE: 17.3365 - MAE: 0.7498\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7412 - MAPE: 17.1449 - MAE: 0.7412\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7076 - MAPE: 16.6972 - MAE: 0.7076\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7106 - MAPE: 16.6332 - MAE: 0.7106\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7479 - MAPE: 17.2939 - MAE: 0.7479\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7752 - MAPE: 17.5722 - MAE: 0.7752\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6723 - MAPE: 16.3609 - MAE: 0.6723\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7512 - MAPE: 17.3024 - MAE: 0.7512\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7326 - MAPE: 17.0089 - MAE: 0.7326\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7259 - MAPE: 16.8596 - MAE: 0.7259\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7506 - MAPE: 17.3420 - MAE: 0.7506\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7311 - MAPE: 17.0863 - MAE: 0.7311\n"
     ]
    }
   ],
   "source": [
    "# Initialize prog. learner and populate with one task.\n",
    "pl = build_representation_ensembler(d, num_tasks = 1, verbose_decide = True)\n",
    "pl.add_task(X = X_train, y = y_train, task_id = 0, transformer_voter_decider_split = [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single, deterministic task (n = 10000 and d = 10):\n",
      "MAPE = 0.3355852100974681\n",
      "MAE = 1.2635127186724202\n"
     ]
    }
   ],
   "source": [
    "# Predict on this task.\n",
    "y_pred = pl.predict(X_test, task_id = 0)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Single, deterministic task (n = %d and d = %d):\" % (n_train, d))\n",
    "print(\"MAPE =\", mape)\n",
    "print(\"MAE =\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check 2\n",
    "In this task, we transform by applying a random linear transform followed by an element-wise square. We vote by a random linear combination and add noise for 3 separate tasks. We train an honest learner on just one task with less data, and a progressive learner with data from all 3 tasks. We wish to show that when all three tasks have the same transformer, the progressive learner wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_small = 100\n",
    "n_train_large = 3000\n",
    "n_test = 1000\n",
    "sigma = 1\n",
    "d = 10\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "# We'll use the same (true) transformer for all three tasks, but different voters.\n",
    "rep_dim = 10 # The matrix will be invertible with high probabability.\n",
    "W = (1 / d) * np.random.randn(rep_dim, d)\n",
    "b = np.random.randn(rep_dim)\n",
    "def transform(X):\n",
    "    return np.square(np.dot(X, W.T) + b)\n",
    "def vote_with(v):\n",
    "    return lambda X: np.dot(X, v)\n",
    "data = []\n",
    "\n",
    "# Task 0.\n",
    "v0 = np.random.randn(rep_dim)\n",
    "X, y = generate_data(n_train_small + n_test, d, transform, vote_with(v0), sigma = sigma)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=n_test)\n",
    "data.append((X_train, y_train))\n",
    "\n",
    "# Task 1.\n",
    "v1 = np.random.randn(rep_dim)\n",
    "X, y = generate_data(n_train_large, d, transform, vote_with(v1), sigma = sigma)\n",
    "data.append((X, y))\n",
    "\n",
    "# Task 2.\n",
    "v2 = np.random.randn(rep_dim)\n",
    "X, y = generate_data(n_train_large, d, transform, vote_with(v2), sigma = sigma)\n",
    "data.append((X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-task Honest Learner\n",
    "pl_single = build_knn_neural_neural(d, verbose_transform = False, verbose_vote = False)\n",
    "X, y = data[0]\n",
    "pl_single.add_task(X = X, y = y, task_id = 0, transformer_voter_decider_split = [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multitask Learner (predictions)\n",
    "pl_pred = build_predictive_ensembler(d, verbose_transform = False, verbose_vote = False)\n",
    "for j in range(3):\n",
    "    print(\"Fitting task\", j)\n",
    "    X, y = data[j]\n",
    "    pl_pred.add_task(X = X, y = y, task_id = j, transformer_voter_decider_split = [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multitask Learner (representations)\n",
    "pl_rep = build_representation_ensembler(d, verbose_transform = False, verbose_vote = False)\n",
    "for j in range(3):\n",
    "    print(\"Fitting task\", j)\n",
    "    X, y = data[j]\n",
    "    pl_rep.add_task(X = X, y = y, task_id = j, transformer_voter_decider_split = [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl_single' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6ce71163dda2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmae_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmape_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl_single' is not defined"
     ]
    }
   ],
   "source": [
    "# Single task\n",
    "y_pred_single = pl_single.predict(X_test, task_id = 0)\n",
    "mae_single = mean_absolute_error(y_test, y_pred_single)\n",
    "mape_single = mean_absolute_percentage_error(y_test, y_pred_single)\n",
    "\n",
    "# Prediction Ensembler\n",
    "y_pred = pl_pred.predict(X_test, task_id = 0)\n",
    "mae_pred = mean_absolute_error(y_test, y_pred)\n",
    "mape_pred = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "# Representation Ensembler\n",
    "y_pred = pl_rep.predict(X_test, task_id = 0)\n",
    "mae_rep = mean_absolute_error(y_test, y_pred)\n",
    "mape_rep = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(\"Three tasks with a shared transformer:\")\n",
    "print(\"Training size for Task 0:\", n_train_small)\n",
    "print(\"Training size for Tasks 1 and 2:\", n_train_large)\n",
    "print(\"Dimensionality:\", d)\n",
    "print(\"Noise (var of y | x):\", sigma)\n",
    "print(\"MAE/MAPE on Task 0 of single-task honest learner:\", mae_single, mape_single)\n",
    "print(\"MAE/MAPE on Task 0 of (predictive) progressive learner:\", mae_pred, mape_pred)\n",
    "print(\"MAE/MAPE Transfer Efficiency:\", mae_single / mae_pred, mape_single / mape_pred)\n",
    "print(\"MAE/MAPE on Task 0 of (predictive) progressive learner:\", mae_rep, mape_rep)\n",
    "print(\"MAE/MAPE Transfer Efficiency:\", mae_single / mae_rep, mape_single / mape_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check 3\n",
    "In this task, we try to show a case in which concatenating representations might work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_small = 50\n",
    "n_train_large = 2000\n",
    "n_test = 1000\n",
    "sigma = 1\n",
    "d = 10\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# We'll use 2 transformers for task 1 and 2, and combine them for task 0.\n",
    "rep_dim = 10 # The matrix will be invertible with high probabability.\n",
    "W1 = (1 / d) * np.random.randn(rep_dim, d)\n",
    "b1 = np.random.randn(rep_dim)\n",
    "W2 = (1 / d) * np.random.randn(rep_dim, d)\n",
    "b2 = np.random.randn(rep_dim)\n",
    "v = np.random.randn(rep_dim)\n",
    "def transform1(X):\n",
    "    return np.square(np.dot(X, W1.T) + b2)\n",
    "def transform2(X):\n",
    "    return np.tanh(np.dot(X, W2.T) + b2)\n",
    "def transform0(X):\n",
    "    return 0.5*transform1(X) + 0.5*transform2(X)\n",
    "def vote(X):\n",
    "    return np.dot(X, v)\n",
    "data = []\n",
    "\n",
    "# Task 0.\n",
    "X, y = generate_data(n_train_small + n_test, d, transform0, vote, sigma = sigma)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=n_test)\n",
    "data.append((X_train, y_train))\n",
    "\n",
    "# Task 1.\n",
    "X, y = generate_data(n_train_large, d, transform1, vote, sigma = sigma)\n",
    "data.append((X, y))\n",
    "\n",
    "# Task 2.\n",
    "X, y = generate_data(n_train_large, d, transform2, vote, sigma = sigma)\n",
    "data.append((X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-task Honest Learner\n",
    "pl_single = build_knn_neural_neural(d, verbose_transform = False, verbose_vote = False)\n",
    "X, y = data[0]\n",
    "pl_single.add_task(X = X, y = y, task_id = 0, transformer_voter_decider_split = [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multitask Learner\n",
    "pl = build_knn_neural_neural(d, verbose_transform = False, verbose_vote = False)\n",
    "for j in range(3):\n",
    "    print(\"Fitting task\", j)\n",
    "    X, y = data[j]\n",
    "    pl.add_task(X = X, y = y, task_id = j, transformer_voter_decider_split = [0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_single = pl_single.predict(X_test, task_id = 0)\n",
    "mae_single = mean_absolute_error(y_test, y_pred_single)\n",
    "mape_single = mean_absolute_percentage_error(y_test, y_pred_single)\n",
    "\n",
    "y_pred = pl.predict(X_test, task_id = 0)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(\"Three tasks with a transformer being a function of the others:\")\n",
    "print(\"Training size for Task 0:\", n_train_small)\n",
    "print(\"Training size for Tasks 1 and 2:\", n_train_large)\n",
    "print(\"Dimensionality:\", d)\n",
    "print(\"Noise (var of y | x):\", sigma)\n",
    "print(\"MAE/MAPE on Task 0 of single-task honest learner:\", mae_single, mape_single)\n",
    "print(\"MAE/MAPE on Task 0 of progressive learner:\", mae, mape)\n",
    "print(\"MAE/MAPE Transfer Efficiency:\", mae_single / mae, mape_single / mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
