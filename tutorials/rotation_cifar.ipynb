{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation CIFAR Experiment\n",
    "\n",
    "This experiment will use images from the **CIFAR-100** database (https://www.cs.toronto.edu/~kriz/cifar.html) and showcase the backward transfer efficiency of algorithms in the **Progressive Learning** project (https://github.com/neurodata/progressive-learning) as the images are rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages for experiment\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage.transform import rotate\n",
    "from scipy import ndimage\n",
    "from skimage.util import img_as_ubyte\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble.forest import _generate_sample_indices\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import product\n",
    "import keras\n",
    "from keras import layers\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (forest.py, line 93)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/srahul/myVenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3417\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-2-d49c967e26cf>\"\u001b[0m, line \u001b[1;32m4\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from proglearn.forest import LifelongClassificationForest\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m991\u001b[0m, in \u001b[1;35m_find_and_load\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m971\u001b[0m, in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m914\u001b[0m, in \u001b[1;35m_find_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1342\u001b[0m, in \u001b[1;35mfind_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1316\u001b[0m, in \u001b[1;35m_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1297\u001b[0m, in \u001b[1;35m_legacy_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m414\u001b[0m, in \u001b[1;35mspec_from_loader\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m649\u001b[0m, in \u001b[1;35mspec_from_file_location\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen zipimport>\"\u001b[0m, line \u001b[1;32m191\u001b[0m, in \u001b[1;35mget_filename\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen zipimport>\"\u001b[0m, line \u001b[1;32m713\u001b[0m, in \u001b[1;35m_get_module_code\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<frozen zipimport>\"\u001b[0;36m, line \u001b[0;32m647\u001b[0;36m, in \u001b[0;35m_compile_source\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/srahul/myVenv/lib/python3.8/site-packages/proglearn-0.0.1-py3.8.egg/proglearn/forest.py\"\u001b[0;36m, line \u001b[0;32m93\u001b[0m\n\u001b[0;31m    self.lf = LifelongClassificationForest(\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "# Import the progressive learning packages\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.network import LifelongClassificationNetwork\n",
    "from proglearn.forest import LifelongClassificationForest\n",
    "from proglearn.deciders import SimpleAverage\n",
    "from proglearn.transformers import TreeClassificationTransformer, NeuralClassificationTransformer \n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter\n",
    "\n",
    "# Create array to store errors\n",
    "errors_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized selection of training and testing subsets\n",
    "def cross_val_data(data_x, data_y, total_cls=10):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]\n",
    "    \n",
    "    \n",
    "    for i in range(total_cls):\n",
    "        indx = idx[i]#np.roll(idx[i],(cv-1)*500)\n",
    "        random.shuffle(indx)\n",
    "        \n",
    "        if i==0:\n",
    "            train_x1 = x[indx[0:250],:]\n",
    "            train_x2 = x[indx[250:500],:]\n",
    "            train_y1 = y[indx[0:250]]\n",
    "            train_y2 = y[indx[250:500]]\n",
    "            \n",
    "            test_x = x[indx[500:600],:]\n",
    "            test_y = y[indx[500:600]]\n",
    "        else:\n",
    "            train_x1 = np.concatenate((train_x1, x[indx[0:250],:]), axis=0)\n",
    "            train_x2 = np.concatenate((train_x2, x[indx[250:500],:]), axis=0)\n",
    "            train_y1 = np.concatenate((train_y1, y[indx[0:250]]), axis=0)\n",
    "            train_y2 = np.concatenate((train_y2, y[indx[250:500]]), axis=0)\n",
    "            \n",
    "            test_x = np.concatenate((test_x, x[indx[500:600],:]), axis=0)\n",
    "            test_y = np.concatenate((test_y, y[indx[500:600]]), axis=0)\n",
    "        \n",
    "    return train_x1, train_y1, train_x2, train_y2, test_x, test_y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "The progressive-learning repo contains two main algorithms, **Lifelong Learning Forests** (L2F) and **Lifelong Learning Network** (L2N), the difference being that L2F uses the Uncertainty Forest transformer while L2N uses deep neural networks. Both algorithms, unlike LwF, EWC, Online_EWC, and SI, have been shown to achieve both forward and backward knowledge transfer. Either algorithm can be chosen for the purpose of this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "If the chosen algorithm is trained on both straight up-and-down CIFAR images and rotated CIFAR images, rather than just straight up-and-down CIFAR images, will it perform better (achieve a higher backward transfer efficiency) when tested on straight up-and-down CIFAR images? How does the angle at which training images are rotated affect these results?\n",
    "\n",
    "At a rotation angle of 0 degrees, the rotated images simply provide additional straight up-and-down CIFAR training data, so the backward transfer efficiency at this angle show whether or not the chosen algorithm can even achieve backward knowledge transfer. As the angle of rotation increases, the rotated images become less and less similar to the original dataset, so the backward transfer efficiency should logically decrease, while still being above 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chooses model to use as transformer\n",
    "def choose_transformer(train_x1, test_x, test_y, tmp_data):\n",
    "    \n",
    "    # Deep Neural Networks model is used as transformer\n",
    "    if model == \"dnn\":\n",
    "\n",
    "        default_transformer_class = NeuralClassificationTransformer\n",
    "\n",
    "        network = keras.Sequential()\n",
    "        network.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=np.shape(train_x1)[1:]))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=254, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "\n",
    "        network.add(layers.Flatten())\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "        default_transformer_kwargs = {\"network\" : network, \n",
    "                                      \"euclidean_layer_idx\" : -2,\n",
    "                                      \"num_classes\" : 10,\n",
    "                                      \"optimizer\" : keras.optimizers.Adam(3e-4)\n",
    "                                     }\n",
    "\n",
    "        default_voter_class = KNNClassificationVoter\n",
    "        default_voter_kwargs = {\"k\" : int(np.log2(len(train_x1)))}\n",
    "\n",
    "        default_decider_class = SimpleAverage\n",
    "\n",
    "    # Uncertainty Forest model is used as transformer\n",
    "    elif model == \"uf\":\n",
    "\n",
    "        train_x1 = train_x1.reshape((train_x1.shape[0], train_x1.shape[1] * train_x1.shape[2] * train_x1.shape[3]))\n",
    "        tmp_data = tmp_data.reshape((tmp_data.shape[0], tmp_data.shape[1] * tmp_data.shape[2] * tmp_data.shape[3]))\n",
    "        test_x = test_x.reshape((test_x.shape[0], test_x.shape[1] * test_x.shape[2] * test_x.shape[3]))\n",
    "\n",
    "        default_transformer_class = TreeClassificationTransformer\n",
    "        # Max depth of tree is set\n",
    "        default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : 5}}\n",
    "\n",
    "        default_voter_class = TreeClassificationVoter\n",
    "        default_voter_kwargs = {}\n",
    "\n",
    "        default_decider_class = SimpleAverage\n",
    "        \n",
    "    return (train_x1, test_x, tmp_data, default_transformer_class, default_transformer_kwargs, \n",
    "            default_voter_class, default_voter_kwargs, default_decider_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the experiments\n",
    "def LF_experiment(data_x, data_y, angle, model, granularity, reps=1, ntrees=29, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "    \n",
    "    errors = np.zeros(2)\n",
    "    \n",
    "    with tf.device('/gpu:'+str(int(angle //  granularity) % 4)):\n",
    "        for rep in range(reps):\n",
    "            print(\"rep:{}\".format(rep))\n",
    "            train_x1, train_y1, train_x2, train_y2, test_x, test_y = cross_val_data(data_x, data_y, total_cls=10)\n",
    "\n",
    "            # change data angle for second task\n",
    "            tmp_data = train_x2.copy()\n",
    "            _tmp_ = np.zeros((32,32,3), dtype=int)\n",
    "            total_data = tmp_data.shape[0]\n",
    "\n",
    "            for i in range(total_data):\n",
    "                tmp_ = image_aug(tmp_data[i],angle)\n",
    "                tmp_data[i] = tmp_\n",
    "                \n",
    "            # Call choose_transformer function\n",
    "            (train_x1, test_x, tmp_data, default_transformer_class, default_transformer_kwargs, \n",
    "             default_voter_class, default_voter_kwargs, default_decider_class) = choose_transformer(train_x1, test_x, \n",
    "                                                                                                    test_y, tmp_data)\n",
    "            \n",
    "            if model == \"uf\":\n",
    "                progressive_learner = LifelongClassificationForest(default_transformer_class = default_transformer_class, \n",
    "                                                         default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                                         default_voter_class = default_voter_class,\n",
    "                                                         default_voter_kwargs = default_voter_kwargs,\n",
    "                                                         default_decider_class = default_decider_class)\n",
    "            elif model == \"dnn\":\n",
    "                progressive_learner = LifelongClassificationNetwork(default_transformer_class = default_transformer_class, \n",
    "                                                         default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                                         default_voter_class = default_voter_class,\n",
    "                                                         default_voter_kwargs = default_voter_kwargs,\n",
    "                                                         default_decider_class = default_decider_class)\n",
    "\n",
    "            progressive_learner.add_task(\n",
    "                X = train_x1, \n",
    "                y = train_y1,\n",
    "                # 67% of the data is used for transformers, 33% is used for voters\n",
    "                transformer_voter_decider_split = [0.67, 0.33, 0],\n",
    "                decider_kwargs = {\"classes\" : np.unique(train_y1)}\n",
    "            )\n",
    "\n",
    "            progressive_learner.add_transformer(\n",
    "                X = tmp_data, \n",
    "                y = train_y2,\n",
    "                transformer_data_proportion = 1,\n",
    "                backward_task_ids = [0]\n",
    "            )\n",
    "\n",
    "            llf_task1=progressive_learner.predict(test_x, task_id=0)\n",
    "            llf_single_task=progressive_learner.predict(test_x, task_id=0, transformer_ids=[0])\n",
    "\n",
    "            errors[1] = errors[1]+(1 - np.mean(llf_task1 == test_y))\n",
    "            errors[0] = errors[0]+(1 - np.mean(llf_single_task == test_y))\n",
    "    \n",
    "    errors = errors/reps\n",
    "    print(\"Errors For Angle {}: {}\".format(angle, errors))\n",
    "    \n",
    "    return(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotates the image by the given angle and zooms in to remove unnecessary white space at the corners\n",
    "def image_aug(pic, angle, centroid_x=23, centroid_y=23, win=16, scale=1.45):\n",
    "    im_sz = int(np.floor(pic.shape[0]*scale))\n",
    "    pic_ = np.uint8(np.zeros((im_sz,im_sz,3),dtype=int))\n",
    "    \n",
    "    pic_[:,:,0] = ndimage.zoom(pic[:,:,0],scale)\n",
    "    \n",
    "    pic_[:,:,1] = ndimage.zoom(pic[:,:,1],scale)\n",
    "    pic_[:,:,2] = ndimage.zoom(pic[:,:,2],scale)\n",
    "    \n",
    "    image_aug = rotate(pic_, angle, resize=False)\n",
    "    #print(image_aug.shape)\n",
    "    image_aug_ = image_aug[centroid_x-win:centroid_x+win,centroid_y-win:centroid_y+win,:]\n",
    "    \n",
    "    return img_as_ubyte(image_aug_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "Hyperparameters determine how the model will run. Changing the value of `model` to `\"uf\"` will run the L2F algorithm, while `\"dnn\"` will run the L2N algorithm.\n",
    "\n",
    "`granularity` refers to the amount by which the angle will be increased each time. Setting this value at 1 will cause the algorithm to test every whole number rotation angle between 0 and 180 degrees.\n",
    "\n",
    "`reps` refers to the number of repetitions tested for each angle of rotation. For each repetition, the data is randomly resampled/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN HYPERPARAMS ###\n",
    "model = \"uf\"\n",
    "granularity = 45\n",
    "reps = 5\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and reshapes data sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "data_x = np.concatenate([X_train, X_test])\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the experiment at a new angle of rotation\n",
    "def perform_angle(angle):\n",
    "    error_list = LF_experiment(data_x, data_y, angle, model, granularity, reps=reps, ntrees=16, acorn=1)\n",
    "    \n",
    "    return(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run L2N\n",
    "if model == \"dnn\":\n",
    "    for angle_adder in range(0, 181, granularity * 4):\n",
    "        angles = angle_adder + np.arange(0, granularity * 4, granularity)\n",
    "        with Pool(4) as p:\n",
    "            errors_array = p.map(perform_angle, angles)\n",
    "            \n",
    "# Run L2F\n",
    "elif model == \"uf\":\n",
    "    angles = np.arange(0, 181, granularity)\n",
    "    with Pool(8) as p:\n",
    "        errors_array.append(p.map(perform_angle, angles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation CIFAR Plot\n",
    "\n",
    "This section takes the results of the experiment and plots the backward transfer efficiency against the angle of rotation for the images in **CIFAR-100**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose which algorithms to plot\n",
    "alg_name = ['L2F']#['L2N','L2F','LwF','EWC','Online_EWC','SI']\n",
    "angles = np.arange(0,181,45)\n",
    "tes = [[] for _ in range(len(alg_name))]\n",
    "\n",
    "for algo_no,alg in enumerate(alg_name):\n",
    "    for angle in angles:\n",
    "        orig_error, transfer_error = errors_array[int(angle/granularity)]\n",
    "        tes[algo_no].append(orig_error / transfer_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose which color to make each algorithm's results\n",
    "clr = [\"#00008B\"]#[\"#00008B\", \"#e41a1c\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#CCCC00\"]\n",
    "c = sns.color_palette(clr, n_colors=len(clr))\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "#Plot the data\n",
    "for alg_no,alg in enumerate(alg_name):\n",
    "    if alg_no<2:\n",
    "        ax.plot(angles,tes[alg_no], c=c[alg_no], label=alg_name[alg_no], linewidth=3)\n",
    "    else:\n",
    "        ax.plot(angles,tes[alg_no], c=c[alg_no], label=alg_name[alg_no])\n",
    "\n",
    "#ax.set_yticks([.9,.95, 1, 1.05,1.11])\n",
    "#ax.set_ylim([0.85,1.13])\n",
    "ax.set_xticks([0,30,60,90,120,150,180])\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.set_xlabel('Angle of Rotation (Degrees)', fontsize=24)\n",
    "ax.set_ylabel('Backward Transfer Efficiency', fontsize=24)\n",
    "ax.set_title(\"Rotation Experiment\", fontsize = 24)\n",
    "right_side = ax.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "plt.tight_layout()\n",
    "#x.legend(fontsize = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQs\n",
    "\n",
    "### Why am I getting an \"out of memory\" error?\n",
    "`Pool(8)` in the previous cell allows for parallel processing, so the number within the parenthesis should be, at max, the number of cores in the device on which this notebook is being run. Even if a warning is produced, the results of the experimented should not be affected.\n",
    "\n",
    "### Why is this taking so long to run? How can I speed it up to see if I am getting the expected outputs?\n",
    "Changing the maximum tree depth does not affect the runtime much. Decreasing the value of `reps`, decreasing the value of `num_transformers`, or increasing the value of `granularity` will all decrease runtime at the cost of noisier results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
