{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recruitment Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing lifelong learning algorithms, prior work has involved two main approaches: building and reallocating. Building involves adding new resources to support the arrival of new data, whereas reallocation involves compression of representations to make room for new ones. However, biologically, there is a spectrum between these two modes.\n",
    "\n",
    "In order to examine whether current resources could be better leveraged, we test a range of approaches: **recruitment** of the best-performing existing trees, **building** new trees completely (the default approach used by lifelong classification forests), ignoring all prior trees (essentially an uncertainty forest), and a **hybrid** between building and recruitment.\n",
    "\n",
    "This experiment examines the performance of these four approaches based on the available training sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(action='once')\n",
    "\n",
    "import functions.recruitment_functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This notebook tutorial uses functions stored externally within `functions/recruitment_functions.py` to simplify presentation of code. These functions are imported above, along with other libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 10x10 Tasks\n",
    "\n",
    "The classification problem that we examine in this tutorial makes use of the CIFAR 10x10 dataset. This dataset contains \n",
    "\n",
    "\n",
    "DESCRIBE CIFAR LATER HERE!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data \n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "data_x = np.concatenate([X_train, X_test])\n",
    "data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2] * data_x.shape[3]))\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]\n",
    "\n",
    "# maybe plot an example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the CIFAR 10x10 dataset, we can prepare to run the experiment. The function for running the experiment, `experiment`, can be found within `functions/recruitment_functions.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first declare the hyperparameters to be used for the experiment, which are as follows:\n",
    "- `ntrees`: describe\n",
    "- `reps`: describe\n",
    "- `estimation_set`: describe\n",
    "- `num_points_per_task`: describe\n",
    "- `num_points_per_forest`: describe\n",
    "- `task_10_sample`: describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### Main hyperparameters ###\n",
    "############################\n",
    "ntrees = 50\n",
    "reps = 5\n",
    "#reps = 30\n",
    "estimation_set = 0.63\n",
    "num_points_per_task = 5000 # see sort_data\n",
    "num_points_per_forest = 500 # maybe rename this \n",
    "task_10_sample = 10*np.array([10, 50, 100, 200, 350, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replicate the experiment found in [Vogelstein, et al. (2020)](https://arxiv.org/pdf/2004.12908.pdf), we use the hyperparameter values above.\n",
    "\n",
    "Essentially, this trains a lifelong forest on the first nine CIFAR 10x10 tasks, where we have `50` trees (`ntrees`) and `500` samples (`num_points_per_forest`) for each set.\n",
    "\n",
    "For the 10th task, we use training sample sizes ranging from `100` to `5000` (`task_10_sample`) and obtain generalization errors for each of the following approaches:\n",
    "1. **Building (default for lifelong forests)**, which involves training `ntrees=50` new trees,\n",
    "2. **Uncertainty forest**, which ignores all prior trees,\n",
    "3. **Recruiting**, which selects the `ntrees=50` (out of all 450 existing trees) that perform best on the newly introduced 10th task, and\n",
    "4. **Hybrid**, which both builds `ntrees/2=25` new trees and recruits the `ntrees/2=25` best-performing trees.\n",
    "\n",
    "Let's call our `experiment` function and give it a run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing 100 samples for 0 th rep\n",
      "doing 100 samples for 1 th rep\n",
      "doing 100 samples for 2 th rep\n",
      "doing 100 samples for 3 th rep\n",
      "doing 100 samples for 4 th rep\n",
      "doing 500 samples for 0 th rep\n",
      "doing 500 samples for 1 th rep\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-7f233bd085e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_accuracy_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_accuracy_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points_per_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_points_per_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_10_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/progressive-learning/docs/tutorials/functions/recruitment_functions.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(data_x, data_y, ntrees, reps, estimation_set, num_points_per_task, num_points_per_forest, task_10_sample)\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0mnum_transformers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntrees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                         \u001b[0mtransformer_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"max_depth\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimation_sample_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                         \u001b[0mvoter_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"classes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"finite_sample_correction\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                         \u001b[0;31m#decider_kwargs={\"classes\": np.unique(cur_y)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/proglearn/progressive_learner.py\u001b[0m in \u001b[0;36madd_transformer\u001b[0;34m(self, X, y, transformer_data_proportion, transformer_voter_data_idx, transformer_id, num_transformers, transformer_class, transformer_kwargs, voter_class, voter_kwargs, backward_task_ids)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0mtransformer_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0mdefault_voter_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoter_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                 \u001b[0mdefault_voter_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvoter_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             )\n\u001b[1;32m    535\u001b[0m             \u001b[0mvoter_data_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_voter_data_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_data_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/proglearn/progressive_learner.py\u001b[0m in \u001b[0;36mset_transformer\u001b[0;34m(self, transformer_id, transformer, transformer_data_idx, transformer_class, transformer_kwargs, default_voter_class, default_voter_kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# Type check y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             self._append_transformer(\n\u001b[0;32m--> 321\u001b[0;31m                 \u001b[0mtransformer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtransformer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/proglearn/transformers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# define the ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_fitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_acc_dict, std_acc_dict = fn.experiment(data_x, data_y, ntrees, reps, estimation_set, num_points_per_task, num_points_per_forest, task_10_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (mean_accuracy_dict,std_accuracy_dict)\n",
    "\n",
    "with open('result/recruitment_exp_'+str(num_points_per_forest)+'.pickle','wb') as f:\n",
    "    pickle.dump(summary,f)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should see L2F outperform others except @ 5k training samples: \"relative performance depends on available resources and sample size\"\n",
    "\n",
    "future work: \"investigate optimal strtegies or determining how to optimally leverage existing resources given a new task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "\n",
    "ns = task_10_sample\n",
    "\n",
    "colors = sns.color_palette('Set1', n_colors=mean_acc_dict.shape[0]+2)\n",
    "labels = ['hybrid', 'L2F (building)', 'recruiting', 'UF (new)']\n",
    "\n",
    "for i, error_ in enumerate(mean_acc_dict):\n",
    "    ax.plot(ns, mean_acc_dict[i], c=colors[i], label=labels[i])\n",
    "    \n",
    "#for i, error_ in enumerate(mean_error):\n",
    "    #ax.plot(ns, mean_error[i], c=colors[i+1-adjust], label=labels[i])\n",
    "    #ax.fill_between(ns, \n",
    "    #        mean_error[i] + 1.96*std_error[i], \n",
    "    #        mean_error[i] - 1.96*std_error[i], \n",
    "    #        where=mean_error[i] + 1.96*std_error[i] >= mean_error[i] - 1.96*std_error[i], \n",
    "    #        facecolor=colors[i+1-adjust], \n",
    "    #        alpha=0.15,\n",
    "    #        interpolate=False)\n",
    "\n",
    "#ax.plot(ns, mean_error[-1], c=colors[0], label=labels[-1])\n",
    "#ax.fill_between(ns, \n",
    "    #    mean_error[-1] + 1.96*std_error[-1], \n",
    "    #    mean_error[-1] - 1.96*std_error[-1], \n",
    "    #    where=mean_error[-1] + 1.96*std_error[i] >= mean_error[-1] - 1.96*std_error[-1], \n",
    "    #    facecolor=colors[0], \n",
    "    #    alpha=0.15,\n",
    "    #    interpolate=False)\n",
    "\n",
    "\n",
    "ax.set_title(\"CIFAR Recruitment Experiment\",fontsize=30)\n",
    "ax.set_ylabel('Generalization Error (Task 10)', fontsize=28)\n",
    "ax.set_xlabel('Number of Task 10 Samples', fontsize=30)\n",
    "ax.tick_params(labelsize=28)\n",
    "ax.set_ylim(0.400, 0.900)\n",
    "ax.set_xticks([500, 2000, 5000])\n",
    "ax.set_yticks([0.35, 0.45, 0.55, 0.65, 0.75, 0.85])\n",
    "\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "right_side = ax.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
