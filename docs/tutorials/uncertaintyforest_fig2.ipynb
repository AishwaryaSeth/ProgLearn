{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Overview\n",
    "This tutorial (`uncertaintyforest_fig2.ipynb`) will further explain the UncertaintyForest class by allowing you to generate Figure 2 from [this paper](https://arxiv.org/pdf/1907.00325.pdf) on your own machine. \n",
    "\n",
    "If you haven't seen it already, take a look at other tutorials to setup and install the progressive learning package `Installation-and-Package-Setup-Tutorial.ipynb`\n",
    "\n",
    "# Analyzing the UncertaintyForest Class by Reproducing Figure 2\n",
    "## *Goal: Run the UncertaintyForest class to produce the results from Figure 2*\n",
    "*Note: Figure 2 refers to Figure 2 from [this paper](https://arxiv.org/pdf/1907.00325.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we'll import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from proglearn.forest import UncertaintyForest\n",
    "from functions.unc_forest_tutorials_functions import generate_data_fig2, CART_estimate, true_cond_entropy, format_func, get_cond_entropy_vs_n, get_cond_entropy_vs_mu, plot_cond_entropy_by_n, plot_cond_entropy_by_mu, plot_fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we'll specify some parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are two sets of parameters.\n",
    "# The first are those that were actually used to produce figure 2.\n",
    "# These take a long time to actually run since there are up to 6000 data points.\n",
    "# Below those, you'll find some testing parameters so that you can see the results more quickly.\n",
    "\n",
    "# Here are the \"Real Parameters\"\n",
    "# mus = [i * 0.5 for i in range(1, 11)] \n",
    "# effect_size = 1\n",
    "# d1 = 1 \n",
    "# d2 = 20 \n",
    "# n1 = 3000 \n",
    "# n2 = 6000 \n",
    "# num_trials = 20 \n",
    "# num_plotted_trials = 10 \n",
    "# sample_sizes_d1 = range(300, 1201, 90) \n",
    "# sample_sizes_d2 = range(500, 3001, 250)\n",
    "\n",
    "# Here are the \"Test Parameters\"\n",
    "mus = [i * 0.5 for i in range(1, 3)] # range of means of the data (x-axis in right column)\n",
    "effect_size = 1 # mu for left column\n",
    "d1 = 1 # data dimensions = 1\n",
    "d2 = 3 # data dimensions = 1, noise dimensions = 19\n",
    "n1 = 100 # number of data points for top row, right column (d1)\n",
    "n2 = 110 # number of data points for bottom row, right column (d2)\n",
    "num_trials = 2 # number of trials to run\n",
    "num_plotted_trials = 2 # the number of \"fainter\" lines to be displayed on the figure\n",
    "sample_sizes_d1 = range(100, 120, 10) # range of data points for top row, left column (d1)\n",
    "sample_sizes_d2 = range(100, 130, 10) # range of data points for bottom row, left column (d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this function, we define how conditional entropy is estimated for the three kinds of learners used in Figure 2 (CART, IRF, and UF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ce(X, y, label):\n",
    "    if label == \"CART\":\n",
    "        return CART_estimate(X, y)\n",
    "    elif label == \"IRF\":\n",
    "        frac_eval = 0.3\n",
    "        irf = CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = 300), \n",
    "                                     method='isotonic', \n",
    "                                     cv = 5)\n",
    "        # X_train, y_train, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=frac_eval)\n",
    "        irf.fit(X_train, y_train)\n",
    "        p = irf.predict_proba(X_eval)\n",
    "        return np.mean(entropy(p.T, base = np.exp(1)))\n",
    "    elif label == \"UF\":\n",
    "        frac_eval = 0.3\n",
    "        uf = UncertaintyForest(n_estimators = n_estimators, tree_construction_proportion = 0.4, kappa = 3.0)\n",
    "        # X_train, y_train, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=frac_eval)\n",
    "        p = uf.predict_proba(X_eval)\n",
    "        return np.mean(entropy(p.T, base = np.exp(1)))\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized Label!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we'll specify which learners we'll compare (by label). Figure 2 uses three different learners specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms used to produce figure 2\n",
    "algos = [\n",
    "    {\n",
    "        'label': 'CART',\n",
    "        'title': 'CART Forest',\n",
    "        'color': \"#1b9e77\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'IRF',\n",
    "        'title': 'Isotonic Reg. Forest',\n",
    "        'color': \"#fdae61\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'UF',\n",
    "        'title': 'Uncertainty Forest',\n",
    "        'color': \"#F41711\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we'll run the code to obtain the results that will be displayed in Figure 2 (4 sets of calculations for 4 subplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code that actually generates data and predictions for conditional entropy vs. n or mu.\n",
    "get_cond_entropy_vs_n(effect_size, d1, num_trials, sample_sizes_d1, algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cond_entropy_vs_mu(n1, d1, num_trials, mus, algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cond_entropy_vs_n(effect_size, d2, num_trials, sample_sizes_d2, algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cond_entropy_vs_mu(n2, d2, num_trials, mus, algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, create Figure 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig2(num_plotted_trials, d1, d2, n1, n2, effect_size, algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}