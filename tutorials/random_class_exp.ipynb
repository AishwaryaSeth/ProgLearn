{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages for experiment\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "from itertools import product\n",
    "from math import log2, ceil \n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Classification Experiment\n",
    "\n",
    "This experiment will use images from the **CIFAR-100** database (https://www.cs.toronto.edu/~kriz/cifar.html) and showcase the classification efficiency of algorithms in the **Progressive Learning** project (https://github.com/neurodata/progressive-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Learning\n",
    "\n",
    "The Progressive Learning project aims to improve program performance on sequentially learned tasks, proposing a lifelong learning approach. It contains two different algorithms: **Lifelong Learning Forests** (L2F) and **Lifelong Learning Network** (L2N). L2F uses Uncertainy Forest as transformers, while L2N uses deep networks. These two algorithms achieve both forward knowledge transfer and backward knowledge transfer, and this experiment is designed to cover both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the progressive learning packages\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "from proglearn.transformers import TreeClassificationTransformer, NeuralClassificationTransformer \n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating data sets\n",
    "\n",
    "The method will separate the data into training and testing subsets. The whole process is randomized to ensure the tests' validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method randomly selects training and testing subsets from the original datasets,\n",
    "# making cross-validation on program results possible.\n",
    "def cross_val_data(data_x, data_y, num_points_per_task, total_task=10, shift=1, slot=0, task=0):\n",
    "    skf = StratifiedKFold(n_splits=6)\n",
    "    for _ in range(shift + 1):\n",
    "        train_idx, test_idx = next(skf.split(data_x, data_y))\n",
    "        \n",
    "    data_x_train, data_y_train = data_x[train_idx], data_y[train_idx]\n",
    "    data_x_test, data_y_test = data_x[test_idx], data_y[test_idx]\n",
    "    \n",
    "    selected_classes = np.random.choice(range(0, 100), total_task)\n",
    "    train_idxs_of_selected_class = np.array([np.where(data_y_train == y_val)[0] for y_val in selected_classes])\n",
    "    num_points_per_class_per_slot = [int(len(train_idxs_of_selected_class[class_idx]) // total_task) \n",
    "                                     for class_idx in range(len(selected_classes))]\n",
    "    selected_idxs = np.concatenate([np.random.choice(train_idxs_of_selected_class[class_idx], \n",
    "                                                     num_points_per_class_per_slot[class_idx]) \n",
    "                                    for class_idx in range(len(selected_classes))])\n",
    "    train_idxs = np.random.choice(selected_idxs, num_points_per_task)\n",
    "    data_x_train = data_x_train[train_idxs]\n",
    "    data_y_train = data_y_train[train_idxs]\n",
    "    \n",
    "    test_idxs_of_selected_class = np.concatenate([np.where(data_y_test == y_val)[0] for y_val in selected_classes])\n",
    "    data_x_test = data_x_test[test_idxs_of_selected_class]\n",
    "    data_y_test = data_y_test[test_idxs_of_selected_class]\n",
    "    \n",
    "    return data_x_train, data_y_train, data_x_test, data_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing experiments\n",
    "\n",
    "This main method integrates both algorithms: L2F and L2N. When executing the experiment, users can choose between L2F and L2N models to generate desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method runs the lifelong learning experiments\n",
    "def L2_experiment(data_x, data_y, ntrees, shift, slot, model, num_points_per_task, acorn=None):\n",
    "    \n",
    "    # construct dataframes\n",
    "    df = pd.DataFrame()\n",
    "    shifts = []\n",
    "    slots = []\n",
    "    accuracies_across_tasks = []\n",
    "    train_times_across_tasks = []\n",
    "    inference_times_across_tasks = []\n",
    "    \n",
    "    # choose Uncertainty Forest as transformer\n",
    "    if model == \"uf\":\n",
    "        default_transformer_class = TreeClassificationTransformer\n",
    "        default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : 30}}\n",
    "        \n",
    "        default_voter_class = TreeClassificationVoter\n",
    "        default_voter_kwargs = {}\n",
    "        \n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "    \n",
    "    # choose deep neural network as transformer\n",
    "    elif model == \"dnn\":\n",
    "        default_transformer_class = NeuralClassificationTransformer\n",
    "        \n",
    "        network = keras.Sequential()\n",
    "        network.add(layers.Conv2D(filters=16, kernel_size=(3, 3),\n",
    "                                  activation='relu', input_shape=np.shape(train_x_task0)[1:]))\n",
    "        network.add(layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
    "                                  strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                                  strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                                  strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.Conv2D(filters=254, kernel_size=(3, 3),\n",
    "                                  strides = 2, padding = \"same\", activation='relu'))\n",
    "\n",
    "        network.add(layers.Flatten())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.Dense(units=10, activation = 'softmax'))\n",
    "        \n",
    "        default_transformer_kwargs = {\"network\" : network, \n",
    "                                      \"euclidean_layer_idx\" : -2,\n",
    "                                      \"num_classes\" : 10,\n",
    "                                      \"optimizer\" : keras.optimizers.Adam(3e-4)}\n",
    "        \n",
    "        default_voter_class = KNNClassificationVoter\n",
    "        default_voter_kwargs = {\"k\" : int(np.log2(num_points_per_task * .33))}\n",
    "        \n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "\n",
    "    # construct the learner with model selected\n",
    "    progressive_learner = ProgressiveLearner(default_transformer_class = default_transformer_class, \n",
    "                                             default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                             default_voter_class = default_voter_class,\n",
    "                                             default_voter_kwargs = default_voter_kwargs,\n",
    "                                             default_decider_class = default_decider_class)\n",
    "    \n",
    "    # randomly separate the training and testing subsets\n",
    "    train_x_task0, train_y_task0, test_x_task0, test_y_task0 = cross_val_data(data_x, data_y,\n",
    "                                                                              num_points_per_task,\n",
    "                                                                              total_task=10,\n",
    "                                                                              shift=shift, slot=slot)\n",
    "    \n",
    "    # training process\n",
    "    train_start_time = time.time()\n",
    "    progressive_learner.add_task(X = train_x_task0, y = train_y_task0,\n",
    "                                 num_transformers = 1 if model == \"dnn\" else ntrees,\n",
    "                                 transformer_voter_decider_split = [0.67, 0.33, 0],\n",
    "                                 decider_kwargs = {\"classes\" : np.unique(train_y_task0)})\n",
    "    train_end_time = time.time()\n",
    "    \n",
    "    # testing process\n",
    "    inference_start_time = time.time()      \n",
    "    task_0_predictions=progressive_learner.predict(test_x_task0, task_id = 0)\n",
    "    inference_end_time = time.time()\n",
    "\n",
    "    # record results\n",
    "    shifts.append(shift)\n",
    "    slots.append(slot)\n",
    "    accuracies_across_tasks.append(np.mean(task_0_predictions == test_y_task0))\n",
    "    train_times_across_tasks.append(train_end_time - train_start_time)\n",
    "    inference_times_across_tasks.append(inference_end_time - inference_start_time)\n",
    "        \n",
    "    # repeating the tasks for 20 times\n",
    "    for task_ii in range(1,20):\n",
    "        \n",
    "        # randomly separate the training and testing subsets\n",
    "        train_x, train_y, _, _ = cross_val_data(data_x, data_y, num_points_per_task,\n",
    "                                                total_task=10, shift=shift, slot=slot, task = task_ii)\n",
    "        \n",
    "        # training process\n",
    "        train_start_time = time.time()\n",
    "        progressive_learner.add_transformer(X = train_x, y = train_y,\n",
    "                                            transformer_data_proportion = 1,\n",
    "                                            num_transformers = 1 if model == \"dnn\" else ntrees,\n",
    "                                            backward_task_ids = [0])\n",
    "        train_end_time = time.time()\n",
    "        \n",
    "        # testing process\n",
    "        inference_start_time = time.time()      \n",
    "        task_0_predictions=progressive_learner.predict(test_x_task0, task_id = 0)\n",
    "        inference_end_time = time.time()\n",
    "        \n",
    "        # record results\n",
    "        shifts.append(shift)\n",
    "        slots.append(slot)\n",
    "        accuracies_across_tasks.append(np.mean(task_0_predictions == test_y_task0))\n",
    "        train_times_across_tasks.append(train_end_time - train_start_time)\n",
    "        inference_times_across_tasks.append(inference_end_time - inference_start_time)\n",
    "        \n",
    "    # finalize dataframes\n",
    "    df['data_fold'] = shifts\n",
    "    df['slot'] = slots\n",
    "    df['accuracy'] = accuracies_across_tasks\n",
    "    df['train_times'] = train_times_across_tasks\n",
    "    df['inference_times'] = inference_times_across_tasks\n",
    "\n",
    "    # save results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing parallel tasks\n",
    "To ensure the experiment's efficiency, this method would make running parallel processes possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method allows multiple lifelong forest experiments to be run at the same time\n",
    "def run_parallel_exp(data_x, data_y, n_trees, model, num_points_per_task, slot=0, shift=1):\n",
    "    if model == \"dnn\":\n",
    "        with tf.device('/gpu:'+str(shift % 4)):\n",
    "            return L2_experiment(data_x, data_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)\n",
    "    else:\n",
    "        return L2_experiment(data_x, data_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing hyperparameters\n",
    "\n",
    "The hyperparameters here, especially the `model`, are used for determining how the experiment will run. `uf` will use the L2F algorithm, while `dnn` will use the L2N algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN HYPERPARAMS ###\n",
    "model = \"uf\" # choose \"uf\" or \"dnn\"\n",
    "num_points_per_task = 500\n",
    "slot_num = 10\n",
    "shift_num = 6\n",
    "alg_num = 1\n",
    "task_num = 20\n",
    "tree_num = 10\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "\n",
    "The CITAR-100 database contains 100 classes of 600 images, each separating into 500 training images and 100 testing images. If the L2F model is chosen, the data will be fitted to the proper shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image datasets from the CIFAR-100 database\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "# modify data shapes for specific model\n",
    "data_x = np.concatenate([X_train, X_test])\n",
    "if model == \"uf\":\n",
    "    data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2] * data_x.shape[3]))\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiment\n",
    "\n",
    "Depending on the chosen model, the experiment will feed the data into proper parallel processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "slot_fold = range(10)\n",
    "\n",
    "# run the L2F model if selected\n",
    "if model == \"uf\":\n",
    "    shift_fold = range(1,7,1)\n",
    "    n_trees = [10]\n",
    "    iterable = product(n_trees,shift_fold, slot_fold)\n",
    "    df_results = Parallel(n_jobs=-1,verbose=1)(\n",
    "        delayed(run_parallel_exp)(\n",
    "            data_x, data_y, ntree, model, num_points_per_task, slot=slot, shift=shift\n",
    "        )\n",
    "        for ntree,shift,slot in iterable\n",
    "    )\n",
    "\n",
    "# run the L2N model if selected\n",
    "elif model == \"dnn\":\n",
    "    \n",
    "    df_results = pd.DataFrame()\n",
    "    \n",
    "    print(\"Performing Stage 1 Shifts\")\n",
    "    for slot in slot_fold:\n",
    "        \n",
    "        def perform_shift(shift):\n",
    "            return run_parallel_exp(data_x, data_y, 0, model, num_points_per_task, slot=slot, shift=shift)\n",
    "    \n",
    "        stage_1_shifts = range(1, 5)\n",
    "        with Pool(4) as p:\n",
    "            df_results = df_results.append(p.map(perform_shift, stage_1_shifts))\n",
    "            \n",
    "    print(\"Performing Stage 2 Shifts\")\n",
    "    for slot in slot_fold:\n",
    "        \n",
    "        def perform_shift(shift):\n",
    "            return run_parallel_exp(data_x, data_y, 0, model, num_points_per_task, slot=slot, shift=shift)\n",
    "    \n",
    "        stage_2_shifts = range(5, 7)\n",
    "        with Pool(4) as p:\n",
    "            df_results = df_results.p.map(perform_shift, stage_2_shifts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the results\n",
    "\n",
    "After the experiment processes finish, the codes below will illustrate the training efficiency and time taken for both training and testing. Though individual computing time might vary, the whole process should be efficient enough to run on any common device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = slot_num * shift_num\n",
    "\n",
    "btes = []\n",
    "train_times = []\n",
    "inference_times = []\n",
    "\n",
    "# read the experiment results\n",
    "for slot in range(slot_num):\n",
    "    for shift in range(shift_num):\n",
    "        \n",
    "        # load the dataframes\n",
    "        df = df_results[shift*slot_num+slot]\n",
    "        \n",
    "        # calculate the backward transfer efficiency\n",
    "        err = 1 - df['accuracy']\n",
    "        bte = err[0] / err\n",
    "        \n",
    "        # record information\n",
    "        btes.append(bte)\n",
    "        train_times.append(df['train_times'])\n",
    "        inference_times.append(df['inference_times'])\n",
    "\n",
    "# calculate the average numbers\n",
    "bte = np.mean(btes, axis = 0)\n",
    "train_time_across_tasks = np.mean(train_times, axis = 0)\n",
    "inference_time_across_tasks = np.mean(inference_times, axis = 0)\n",
    "\n",
    "# setting plot parameters\n",
    "fontsize=22\n",
    "ticksize=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting backward transfer efficiency\n",
    "\n",
    "Backward transfer efficiency (BTE) measures the relative effect of future task data on the performance on a certain task. It is the expected ratio of two risk functions of the learned hypothesis, one with access to the data up to and including the last observation from task t, and the other with access to the entire data sequence.\n",
    "\n",
    "\\begin{align}\n",
    "BTE^t(f_n):= E[R^t(f_n^{<t}) / R^t(f_n)] \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the BTE\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# hide the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax.plot(range(len(bte)), bte, c=\"blue\" if model == \"dnn\" else \"red\", linewidth=3,\n",
    "        linestyle=\"solid\", label = \"L2N\" if model == \"dnn\" else \"L2F\")\n",
    "\n",
    "ax.set_xlabel('Number of Tasks Seen', fontsize=fontsize)\n",
    "ax.set_ylabel('BTE (Task 1)', fontsize=fontsize)\n",
    "ax.set_yticks([1.0, 1.1, 1.2])\n",
    "ax.tick_params(labelsize=ticksize)\n",
    "ax.legend(fontsize=22)\n",
    "\n",
    "# show the BTE plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting time\n",
    "\n",
    "The graph will show how training time and testing time change during the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the recorded times\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# Hide the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "ax.plot(range(len(train_time_across_tasks)), train_time_across_tasks, linewidth=3,\n",
    "        linestyle=\"solid\", label = \"Train Time\")\n",
    "ax.plot(range(len(inference_time_across_tasks)), inference_time_across_tasks, linewidth=3,\n",
    "        linestyle=\"solid\", label = \"Inference Time\")\n",
    "\n",
    "ax.set_xlabel('Number of Tasks Seen', fontsize=fontsize)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=fontsize)\n",
    "ax.tick_params(labelsize=ticksize)\n",
    "ax.legend(fontsize=22)\n",
    "\n",
    "# show the time plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
