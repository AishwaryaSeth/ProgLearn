{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Learning in a Simple Environment\n",
    "## Gaussian XOR and Vaussian N-XOR Experiment\n",
    "\n",
    "fjkdljflakds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import seaborn as sns \n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from math import log2, ceil \n",
    "\n",
    "import sys\n",
    "\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.deciders import SimpleAverage\n",
    "from proglearn.transformers import TreeClassificationTransformer, NeuralClassificationTransformer \n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def get_colors(colors, inds):\n",
    "    c = [colors[i] for i in inds]\n",
    "    return c\n",
    "\n",
    "def generate_2d_rotation(theta=0, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "    \n",
    "    R = np.array([\n",
    "        [np.cos(theta), np.sin(theta)],\n",
    "        [-np.sin(theta), np.cos(theta)]\n",
    "    ])\n",
    "    \n",
    "    return R\n",
    "\n",
    "\n",
    "def generate_gaussian_parity(n, mean=np.array([-1, -1]), cov_scale=1, angle_params=None, k=1, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "        \n",
    "    d = len(mean)\n",
    "    \n",
    "    if mean[0] == -1 and mean[1] == -1:\n",
    "        mean = mean + 1 / 2**k\n",
    "    \n",
    "    mnt = np.random.multinomial(n, 1/(4**k) * np.ones(4**k))\n",
    "    cumsum = np.cumsum(mnt)\n",
    "    cumsum = np.concatenate(([0], cumsum))\n",
    "    \n",
    "    Y = np.zeros(n)\n",
    "    X = np.zeros((n, d))\n",
    "    \n",
    "    for i in range(2**k):\n",
    "        for j in range(2**k):\n",
    "            temp = np.random.multivariate_normal(mean, cov_scale * np.eye(d), \n",
    "                                                 size=mnt[i*(2**k) + j])\n",
    "            temp[:, 0] += i*(1/2**(k-1))\n",
    "            temp[:, 1] += j*(1/2**(k-1))\n",
    "            \n",
    "            X[cumsum[i*(2**k) + j]:cumsum[i*(2**k) + j + 1]] = temp\n",
    "            \n",
    "            if i % 2 == j % 2:\n",
    "                Y[cumsum[i*(2**k) + j]:cumsum[i*(2**k) + j + 1]] = 0\n",
    "            else:\n",
    "                Y[cumsum[i*(2**k) + j]:cumsum[i*(2**k) + j + 1]] = 1\n",
    "                \n",
    "    if d == 2:\n",
    "        if angle_params is None:\n",
    "            angle_params = np.random.uniform(0, 2*np.pi)\n",
    "            \n",
    "        R = generate_2d_rotation(angle_params)\n",
    "        X = X @ R\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('d=%i not implemented!'%(d))\n",
    "       \n",
    "    return X, Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(n_xor, n_nxor, n_test, reps, n_trees, max_depth, acorn=None):\n",
    "    #print(1)\n",
    "    if n_xor==0 and n_nxor==0:\n",
    "        raise ValueError('Wake up and provide samples to train!!!')\n",
    "    \n",
    "    if acorn != None:\n",
    "        np.random.seed(acorn)\n",
    "    \n",
    "    errors = np.zeros((reps,4),dtype=float)\n",
    "    \n",
    "    for i in range(reps):\n",
    "        default_transformer_class = TreeClassificationTransformer\n",
    "        default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : max_depth}}\n",
    "\n",
    "        default_voter_class = TreeClassificationVoter\n",
    "        default_voter_kwargs = {}\n",
    "\n",
    "        default_decider_class = SimpleAverage\n",
    "        default_decider_kwargs = {\"classes\" : np.arange(2)}\n",
    "        progressive_learner = ProgressiveLearner(default_transformer_class = default_transformer_class, \n",
    "                                             default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                             default_voter_class = default_voter_class,\n",
    "                                             default_voter_kwargs = default_voter_kwargs,\n",
    "                                             default_decider_class = default_decider_class,\n",
    "                                             default_decider_kwargs = default_decider_kwargs)\n",
    "        uf = ProgressiveLearner(default_transformer_class = default_transformer_class, \n",
    "                                             default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                             default_voter_class = default_voter_class,\n",
    "                                             default_voter_kwargs = default_voter_kwargs,\n",
    "                                             default_decider_class = default_decider_class,\n",
    "                                             default_decider_kwargs = default_decider_kwargs)\n",
    "        #source data\n",
    "        xor, label_xor = generate_gaussian_parity(n_xor,cov_scale=0.1,angle_params=0)\n",
    "        test_xor, test_label_xor = generate_gaussian_parity(n_test,cov_scale=0.1,angle_params=0)\n",
    "    \n",
    "        #target data\n",
    "        nxor, label_nxor = generate_gaussian_parity(n_nxor,cov_scale=0.1,angle_params=np.pi/2)\n",
    "        test_nxor, test_label_nxor = generate_gaussian_parity(n_test,cov_scale=0.1,angle_params=np.pi/2)\n",
    "    \n",
    "        if n_xor == 0:\n",
    "            progressive_learner.add_task(nxor, label_nxor, num_transformers=n_trees)\n",
    "            \n",
    "            errors[i,0] = 0.5\n",
    "            errors[i,1] = 0.5\n",
    "            \n",
    "            uf_task2=progressive_learner.predict(test_nxor, transformer_ids=[0], task_id=0)\n",
    "            l2f_task2=progressive_learner.predict(test_nxor, task_id=0)\n",
    "            \n",
    "            errors[i,2] = 1 - np.sum(uf_task2 == test_label_nxor)/n_test\n",
    "            errors[i,3] = 1 - np.sum(l2f_task2 == test_label_nxor)/n_test\n",
    "        elif n_nxor == 0:\n",
    "            progressive_learner.add_task(xor, label_xor, num_transformers=n_trees)\n",
    "            \n",
    "            uf_task1=progressive_learner.predict(test_xor, transformer_ids=[0], task_id=0)\n",
    "            l2f_task1=progressive_learner.predict(test_xor, task_id=0)\n",
    "            \n",
    "            errors[i,0] = 1 - np.sum(uf_task1 == test_label_xor)/n_test\n",
    "            errors[i,1] = 1 - np.sum(l2f_task1 == test_label_xor)/n_test\n",
    "            errors[i,2] = 0.5\n",
    "            errors[i,3] = 0.5\n",
    "        else:\n",
    "            progressive_learner.add_task(xor, label_xor, num_transformers=n_trees)\n",
    "            progressive_learner.add_task(nxor, label_nxor, num_transformers=n_trees)\n",
    "            \n",
    "            uf.add_task(xor, label_xor, num_transformers=2*n_trees)\n",
    "            uf.add_task(nxor, label_nxor, num_transformers=2*n_trees)\n",
    "\n",
    "            uf_task1=uf.predict(test_xor, transformer_ids=[0], task_id=0)\n",
    "            l2f_task1=progressive_learner.predict(test_xor, task_id=0)\n",
    "            uf_task2=uf.predict(test_nxor, transformer_ids=[1], task_id=1)\n",
    "            l2f_task2=progressive_learner.predict(test_nxor, task_id=1)\n",
    "            \n",
    "            errors[i,0] = 1 - np.sum(uf_task1 == test_label_xor)/n_test\n",
    "            errors[i,1] = 1 - np.sum(l2f_task1 == test_label_xor)/n_test\n",
    "            errors[i,2] = 1 - np.sum(uf_task2 == test_label_nxor)/n_test\n",
    "            errors[i,3] = 1 - np.sum(l2f_task2 == test_label_nxor)/n_test\n",
    "\n",
    "    return np.mean(errors,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_rep = 1000\n",
    "n_test = 1000\n",
    "n_trees = 10\n",
    "n_xor = (100*np.arange(0.5, 7.25, step=0.25)).astype(int)\n",
    "n_nxor = (100*np.arange(0.5, 7.50, step=0.25)).astype(int)\n",
    "\n",
    "mean_error = np.zeros((4, len(n_xor)+len(n_nxor)))\n",
    "std_error = np.zeros((4, len(n_xor)+len(n_nxor)))\n",
    "\n",
    "mean_te = np.zeros((2, len(n_xor)+len(n_nxor)))\n",
    "std_te = np.zeros((2, len(n_xor)+len(n_nxor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 50 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 75 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 100 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 125 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 150 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 175 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 200 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 225 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 250 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 275 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 300 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 325 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 350 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 375 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 400 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 425 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 450 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 475 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 500 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 525 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 550 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 575 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 600 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 625 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 650 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 675 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 700 xor\n",
      "\n",
      "starting to compute 50 nxor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "exception calling callback for <Future at 0x7f1f30aa7978 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/externals/loky/_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 347, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 780, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 531, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/externals/loky/reusable_executor.py\", line 178, in submit\n",
      "    fn, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 1102, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n",
      "The exit codes of the workers are {SIGKILL(-9)}\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e93f091b6e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             error = np.array(\n\u001b[1;32m     20\u001b[0m                 Parallel(n_jobs=40,verbose=1)(\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_trees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trees\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             )\n\u001b[1;32m     23\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/externals/loky/_base.py\u001b[0m in \u001b[0;36m_invoke_callbacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exception calling callback for %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \"\"\"\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSafeFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_future_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/externals/loky/reusable_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit_resize_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             return super(_ReusablePoolExecutor, self).submit(\n\u001b[0;32m--> 178\u001b[0;31m                 fn, *args, **kwargs)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 raise ShutdownExecutorError(\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "for i,n1 in enumerate(n_xor):\n",
    "    print('starting to compute %s xor\\n'%n1)\n",
    "    error = np.array(\n",
    "        Parallel(n_jobs=-1,verbose=1)(\n",
    "        delayed(experiment)(n1,0,n_test,1,n_trees=n_trees,max_depth=ceil(log2(750))) for _ in range(mc_rep)\n",
    "    )\n",
    "    )\n",
    "    mean_error[:,i] = np.mean(error,axis=0)\n",
    "    std_error[:,i] = np.std(error,ddof=1,axis=0)\n",
    "    mean_te[0,i] = np.mean(error[:,0]/error[:,1])\n",
    "    mean_te[1,i] = np.mean(error[:,2]/error[:,3])\n",
    "    std_te[0,i] = np.std(error[:,0]/error[:,1],ddof=1)\n",
    "    std_te[1,i] = np.std(error[:,2]/error[:,3],ddof=1)\n",
    "    \n",
    "    if n1==n_xor[-1]:\n",
    "        for j,n2 in enumerate(n_nxor):\n",
    "            print('starting to compute %s nxor\\n'%n2)\n",
    "            \n",
    "            error = np.array(\n",
    "                Parallel(n_jobs=40,verbose=1)(\n",
    "                delayed(experiment)(n1,n2,n_test,1,n_trees=n_trees,max_depth=ceil(log2(750))) for _ in range(mc_rep)\n",
    "            )\n",
    "            )\n",
    "            mean_error[:,i+j+1] = np.mean(error,axis=0)\n",
    "            std_error[:,i+j+1] = np.std(error,ddof=1,axis=0)\n",
    "            mean_te[0,i+j+1] = np.mean(error[:,0]/error[:,1])\n",
    "            mean_te[1,i+j+1] = np.mean(error[:,2]/error[:,3])\n",
    "            std_te[0,i+j+1] = np.std(error[:,0]/error[:,1],ddof=1)\n",
    "            std_te[1,i+j+1] = np.std(error[:,2]/error[:,3],ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 50 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 75 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 100 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 125 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 150 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 175 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 200 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 225 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 250 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 275 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 300 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 325 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 350 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 214.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 375 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 400 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 425 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 450 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 475 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 500 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 525 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 550 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 575 xor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to compute 600 xor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,n1 in enumerate(n_xor):\n",
    "    print('starting to compute %s xor\\n'%n1)\n",
    "    error = np.array(\n",
    "        Parallel(n_jobs=-1,verbose=1)(\n",
    "        delayed(experiment)(n1,0,n_test,1,n_trees=n_trees,max_depth=ceil(log2(750))) for _ in range(mc_rep)\n",
    "    )\n",
    "    )\n",
    "    mean_error[:,i] = np.mean(error,axis=0)\n",
    "    std_error[:,i] = np.std(error,ddof=1,axis=0)\n",
    "    mean_te[0,i] = np.mean(error[:,0]/error[:,1])\n",
    "    mean_te[1,i] = np.mean(error[:,2]/error[:,3])\n",
    "    std_te[0,i] = np.std(error[:,0]/error[:,1],ddof=1)\n",
    "    std_te[1,i] = np.std(error[:,2]/error[:,3],ddof=1)\n",
    "    \n",
    "    if n1==n_xor[-1]:\n",
    "        for j,n2 in enumerate(n_nxor):\n",
    "            print('starting to compute %s nxor\\n'%n2)\n",
    "            \n",
    "            error = np.array(\n",
    "                Parallel(n_jobs=-1,verbose=1)(\n",
    "                delayed(experiment)(n1,n2,n_test,1,n_trees=n_trees,max_depth=ceil(log2(750))) for _ in range(mc_rep)\n",
    "            )\n",
    "            )\n",
    "            mean_error[:,i+j+1] = np.mean(error,axis=0)\n",
    "            std_error[:,i+j+1] = np.std(error,ddof=1,axis=0)\n",
    "            mean_te[0,i+j+1] = np.mean(error[:,0]/error[:,1])\n",
    "            mean_te[1,i+j+1] = np.mean(error[:,2]/error[:,3])\n",
    "            std_te[0,i+j+1] = np.std(error[:,0]/error[:,1],ddof=1)\n",
    "            std_te[1,i+j+1] = np.std(error[:,2]/error[:,3],ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './result/mean_xor_nxor.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-25665e1d0a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./result/mean_xor_nxor.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./result/std_xor_nxor.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './result/mean_xor_nxor.pickle'"
     ]
    }
   ],
   "source": [
    "with open('./result/mean_xor_nxor.pickle','wb') as f:\n",
    "    pickle.dump(mean_error,f)\n",
    "    \n",
    "with open('./result/std_xor_nxor.pickle','wb') as f:\n",
    "     pickle.dump(std_error,f)\n",
    "    \n",
    "with open('./result/mean_te_xor_nxor.pickle','wb') as f:\n",
    "     pickle.dump(mean_te,f)\n",
    "    \n",
    "with open('./result/std_te_xor_nxor.pickle','wb') as f:\n",
    "     pickle.dump(std_te,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plotting the result\n",
    "#mc_rep = 50\n",
    "mean_error = unpickle('result/mean_xor_nxor.pickle')\n",
    "std_error = unpickle('result/std_xor_nxor.pickle')\n",
    "\n",
    "n_xor = (100*np.arange(0.5, 7.25, step=0.25)).astype(int)\n",
    "n_nxor = (100*np.arange(0.5, 7.50, step=0.25)).astype(int)\n",
    "\n",
    "n1s = n_xor\n",
    "n2s = n_nxor\n",
    "\n",
    "ns = np.concatenate((n1s, n2s + n1s[-1]))\n",
    "ls=['-', '--']\n",
    "algorithms = ['Uncertainty Forest', 'Lifelong Forest']\n",
    "\n",
    "\n",
    "TASK1='XOR'\n",
    "TASK2='N-XOR'\n",
    "\n",
    "fontsize=35\n",
    "labelsize=27.5\n",
    "\n",
    "colors = sns.color_palette(\"Set1\", n_colors = 2)\n",
    "\n",
    "fig1 = plt.figure(figsize=(8,8))\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "# for i, algo in enumerate(algorithms):\n",
    "ax1.plot(ns, mean_error[0], label=algorithms[0], c=colors[1], ls=ls[np.sum(0 > 1).astype(int)], lw=3)\n",
    "#ax1.fill_between(ns, \n",
    "#        mean_error[0] + 1.96*std_error[0], \n",
    "#        mean_error[0] - 1.96*std_error[0], \n",
    "#        where=mean_error[0] + 1.96*std_error[0] >= mean_error[0] - 1.96*std_error[0], \n",
    "#        facecolor=colors[1], \n",
    "#        alpha=0.15,\n",
    "#        interpolate=True)\n",
    "\n",
    "ax1.plot(ns, mean_error[1], label=algorithms[1], c=colors[0], ls=ls[np.sum(1 > 1).astype(int)], lw=3)\n",
    "#ax1.fill_between(ns, \n",
    "#        mean_error[1] + 1.96*std_error[1, ], \n",
    "#        mean_error[1] - 1.96*std_error[1, ], \n",
    "#        where=mean_error[1] + 1.96*std_error[1] >= mean_error[1] - 1.96*std_error[1], \n",
    "#        facecolor=colors[0], \n",
    "#        alpha=0.15,\n",
    "#        interpolate=True)\n",
    "\n",
    "ax1.set_ylabel('Generalization Error (%s)'%(TASK1), fontname=\"Arial\", fontsize=fontsize)\n",
    "ax1.legend(loc='upper right', fontsize=24, frameon=False)\n",
    "ax1.set_ylim(0.1, 0.21)\n",
    "ax1.set_xlabel('Total Sample Size', fontname=\"Arial\", fontsize=fontsize)\n",
    "ax1.tick_params(labelsize=labelsize)\n",
    "ax1.set_yticks([0.15, 0.2])\n",
    "ax1.set_xticks([250,750,1500])\n",
    "ax1.axvline(x=750, c='gray', linewidth=1.5, linestyle=\"dashed\")\n",
    "\n",
    "right_side = ax1.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax1.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "\n",
    "ax1.text(400, np.mean(ax1.get_ylim()), \"%s\"%(TASK1), fontsize=30)\n",
    "ax1.text(900, np.mean(ax1.get_ylim()), \"%s\"%(TASK2), fontsize=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('./result/figs/generalization_error_xor.pdf',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = unpickle('result/mean_xor_nxor.pickle')\n",
    "std_error = unpickle('result/std_xor_nxor.pickle')\n",
    "\n",
    "algorithms = ['Uncertainty Forest', 'Lifelong Forest']\n",
    "\n",
    "TASK1='XOR'\n",
    "TASK2='N-XOR'\n",
    "\n",
    "fig1 = plt.figure(figsize=(8,8))\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "# for i, algo in enumerate(algorithms):\n",
    "ax1.plot(ns[len(n1s):], mean_error[2, len(n1s):], label=algorithms[0], c=colors[1], ls=ls[1], lw=3)\n",
    "#ax1.fill_between(ns[len(n1s):], \n",
    "#        mean_error[2, len(n1s):] + 1.96*std_error[2, len(n1s):], \n",
    "#        mean_error[2, len(n1s):] - 1.96*std_error[2, len(n1s):], \n",
    "#        where=mean_error[2, len(n1s):] + 1.96*std_error[2, len(n1s):] >= mean_error[2, len(n1s):] - 1.96*std_error[2, len(n1s):], \n",
    "#        facecolor=colors[1], \n",
    "#        alpha=0.15,\n",
    "#        interpolate=True)\n",
    "\n",
    "ax1.plot(ns[len(n1s):], mean_error[3, len(n1s):], label=algorithms[1], c=colors[0], ls=ls[1], lw=3)\n",
    "#ax1.fill_between(ns[len(n1s):], \n",
    "#        mean_error[3, len(n1s):] + 1.96*std_error[3, len(n1s):], \n",
    "#        mean_error[3, len(n1s):] - 1.96*std_error[3, len(n1s):], \n",
    "#        where=mean_error[3, len(n1s):] + 1.96*std_error[3, len(n1s):] >= mean_error[3, len(n1s):] - 1.96*std_error[3, len(n1s):], \n",
    "#        facecolor=colors[0], \n",
    "#        alpha=0.15,\n",
    "#        interpolate=True)\n",
    "\n",
    "ax1.set_ylabel('Generalization Error (%s)'%(TASK2), fontsize=fontsize)\n",
    "ax1.legend(loc='upper right', fontsize=24, frameon=False)\n",
    "#         ax1.set_ylim(-0.01, 0.22)\n",
    "ax1.set_xlabel('Total Sample Size', fontsize=fontsize)\n",
    "ax1.tick_params(labelsize=labelsize)\n",
    "# ax1.set_yticks([0.15, 0.25, 0.35])\n",
    "ax1.set_yticks([0.15, 0.2])\n",
    "ax1.set_xticks([250,750,1500])\n",
    "ax1.axvline(x=750, c='gray', linewidth=1.5, linestyle=\"dashed\")\n",
    "\n",
    "ax1.set_ylim(0.11, 0.21)\n",
    "\n",
    "ax1.set_xlim(-10)\n",
    "right_side = ax1.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax1.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "\n",
    "# ax1.set_ylim(0.14, 0.36)\n",
    "ax1.text(400, np.mean(ax1.get_ylim()), \"%s\"%(TASK1), fontsize=30)\n",
    "ax1.text(900, np.mean(ax1.get_ylim()), \"%s\"%(TASK2), fontsize=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('./result/figs/generalization_error_nxor.pdf',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = unpickle('result/mean_te_xor_nxor.pickle')\n",
    "std_error = unpickle('result/std_te_xor_nxor.pickle')\n",
    "\n",
    "algorithms = ['Forward Transfer', 'Backward Transfer']\n",
    "\n",
    "TASK1='XOR'\n",
    "TASK2='N-XOR'\n",
    "\n",
    "fig1 = plt.figure(figsize=(8,8))\n",
    "ax1 = fig1.add_subplot(1,1,1)\n",
    "\n",
    "ax1.plot(ns, mean_error[0], label=algorithms[0], c=colors[0], ls=ls[0], lw=3)\n",
    "#ax1.fill_between(ns, \n",
    "#        mean_error[0] + 1.96*std_error[0], \n",
    "#        mean_error[0] - 1.96*std_error[0], \n",
    "#        where=mean_error[1] + 1.96*std_error[0] >= mean_error[0] - 1.96*std_error[0], \n",
    "#        facecolor=colors[0], \n",
    "#        alpha=0.15,\n",
    "#        interpolate=True)\n",
    "\n",
    "ax1.plot(ns[len(n1s):], mean_error[1, len(n1s):], label=algorithms[1], c=colors[0], ls=ls[1], lw=3)\n",
    "#ax1.fill_between(ns[len(n1s):], \n",
    "#        mean_error[1, len(n1s):] + 1.96*std_error[1, len(n1s):], \n",
    "#        mean_error[1, len(n1s):] - 1.96*std_error[1, len(n1s):], \n",
    "#        where=mean_error[1, len(n1s):] + 1.96*std_error[1, len(n1s):] >= mean_error[1, len(n1s):] - 1.96*std_error[1, len(n1s):], \n",
    "#        facecolor=colors[0], \n",
    "#        alpha=0.15,\n",
    "#        interpolate=True)\n",
    "\n",
    "ax1.set_ylabel('Transfer Efficiency', fontsize=fontsize)\n",
    "ax1.legend(loc='upper right', fontsize=24, frameon=False)\n",
    "ax1.set_ylim(0.95, 1.42)\n",
    "ax1.set_xlabel('Total Sample Size', fontsize=fontsize)\n",
    "ax1.tick_params(labelsize=labelsize)\n",
    "ax1.set_yticks([1, 1.4])\n",
    "ax1.set_xticks([250,750,1500])\n",
    "ax1.axvline(x=750, c='gray', linewidth=1.5, linestyle=\"dashed\")\n",
    "right_side = ax1.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax1.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "ax1.hlines(1, 50,1500, colors='gray', linestyles='dashed',linewidth=1.5)\n",
    "\n",
    "ax1.text(400, np.mean(ax1.get_ylim()), \"%s\"%(TASK1), fontsize=30)\n",
    "ax1.text(900, np.mean(ax1.get_ylim()), \"%s\"%(TASK2), fontsize=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('./result/figs/TE.pdf',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette('Dark2', n_colors=2)\n",
    "\n",
    "X, Y = generate_gaussian_parity(750, cov_scale=0.1, angle_params=0)\n",
    "Z, W = generate_gaussian_parity(750, cov_scale=0.1, angle_params=np.pi/2)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=get_colors(colors, Y), s=50)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Gaussian XOR', fontsize=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.axis('off')\n",
    "# plt.savefig('./result/figs/gaussian-xor.pdf')\n",
    "\n",
    "#%%\n",
    "colors = sns.color_palette('Dark2', n_colors=2)\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "ax.scatter(Z[:, 0], Z[:, 1], c=get_colors(colors, W), s=50)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Gaussian N-XOR', fontsize=30)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./result/figs/gaussian-nxor.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
